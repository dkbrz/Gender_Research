{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cow = KeyedVectors.load_word2vec_format(\"./models/cow/cow-320.txt\", binary=False)\n",
    "#rou = KeyedVectors.load_word2vec_format(\"./models/roularta/roularta-320.txt\", binary=False)\n",
    "UD = Model('./models/dutch_ud.udpipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_df(filename):\n",
    "    with open('./data/{}.txt'.format(filename)) as f:\n",
    "        return pd.DataFrame(\n",
    "            re.findall('<doc id=\\\"(.*?)\\\" genre=\\\"(.*?)\\\" gender=\\\"(.*?)\\\">\\n(.*?)</doc>', f.read(), re.DOTALL),\n",
    "            columns=['id','genre','gender','text'])\n",
    "    \n",
    "def get_chains(text):\n",
    "    try:\n",
    "        sentences = UD.tokenize(text)\n",
    "        for s in sentences:\n",
    "            UD.tag(s)\n",
    "            UD.parse(s)\n",
    "        conllu = UD.write(sentences, \"conllu\")\n",
    "        rows = conllu.split('\\n')\n",
    "        string_pos, string_rel_type, string_lemmas = '', '', ''\n",
    "        for row in rows:\n",
    "            if '#' not in row and row != '':\n",
    "                row = row.split('\\t')\n",
    "                string_pos = string_pos + row[3] + ' '\n",
    "                string_rel_type = string_rel_type + row[7] + ' '\n",
    "                string_lemmas = string_lemmas + row[2] + ' '\n",
    "            else:\n",
    "                string_pos += '\\n'\n",
    "                string_rel_type += '\\n'\n",
    "                string_lemmas += '\\n'\n",
    "        return (re.sub('\\n+','\\n', string_lemmas[:-1]).strip(), \n",
    "                re.sub('\\n+','\\n', string_pos[:-1]).strip(), \n",
    "                re.sub('\\n+','\\n', string_rel_type[:-1].replace(':','_')).strip())\n",
    "    except:\n",
    "        return '', '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = xml_to_df('GxG_News')\n",
    "twitter = xml_to_df('GxG_Twitter')\n",
    "youtube = xml_to_df('GxG_YouTube')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 234 ms, total: 4min 34s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.array([list(get_chains(i)) for i in news['text'].values]).reshape(news.shape[0],3).T\n",
    "news['lemmatized'], news['pos'], news['rel'] = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...</td>\n",
       "      <td>Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...</td>\n",
       "      <td>NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...</td>\n",
       "      <td>nsubj root case amod obl appos punct case det ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>'t Endezomer  \" het laatste zomerse Meetjeslan...</td>\n",
       "      <td>het Endezomer \" het laat zomers Meetjeslands m...</td>\n",
       "      <td>DET PROPN PUNCT DET ADJ ADJ ADJ NOUN PUNCT X P...</td>\n",
       "      <td>det flat_name punct det amod amod amod nsubj p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>Kamperen bij het kanaal in de Zouten\\nBerendre...</td>\n",
       "      <td>kamperen bij het kanaal in de zouten Berendrec...</td>\n",
       "      <td>VERB ADP DET NOUN ADP DET PROPN PROPN PUNCT DE...</td>\n",
       "      <td>nsubj case det obl case det obl flat_name punc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>F</td>\n",
       "      <td>Sintdreiger komt weg met boete\\nUTRECHT - Hij ...</td>\n",
       "      <td>Sintdreiger komen weg met boete Utrecht - hij ...</td>\n",
       "      <td>PROPN VERB ADV ADP NOUN PROPN PUNCT PRON VERB ...</td>\n",
       "      <td>nsubj root compound_prt case obl appos punct n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>F</td>\n",
       "      <td>‘In Irak zal ik óók het belang van de Nederlan...</td>\n",
       "      <td>‘In Irak zullen ik óóken het belang van de Ned...</td>\n",
       "      <td>ADP PROPN AUX PRON ADJ DET NOUN ADP DET ADJ NO...</td>\n",
       "      <td>case obl aux nsubj advmod det obj case det amo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id genre gender                                               text  \\\n",
       "0  1  news      M  KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...   \n",
       "1  2  news      M  't Endezomer  \" het laatste zomerse Meetjeslan...   \n",
       "2  3  news      M  Kamperen bij het kanaal in de Zouten\\nBerendre...   \n",
       "3  4  news      F  Sintdreiger komt weg met boete\\nUTRECHT - Hij ...   \n",
       "4  5  news      F  ‘In Irak zal ik óók het belang van de Nederlan...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...   \n",
       "1  het Endezomer \" het laat zomers Meetjeslands m...   \n",
       "2  kamperen bij het kanaal in de zouten Berendrec...   \n",
       "3  Sintdreiger komen weg met boete Utrecht - hij ...   \n",
       "4  ‘In Irak zullen ik óóken het belang van de Ned...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...   \n",
       "1  DET PROPN PUNCT DET ADJ ADJ ADJ NOUN PUNCT X P...   \n",
       "2  VERB ADP DET NOUN ADP DET PROPN PROPN PUNCT DE...   \n",
       "3  PROPN VERB ADV ADP NOUN PROPN PUNCT PRON VERB ...   \n",
       "4  ADP PROPN AUX PRON ADJ DET NOUN ADP DET ADJ NO...   \n",
       "\n",
       "                                                 rel  \n",
       "0  nsubj root case amod obl appos punct case det ...  \n",
       "1  det flat_name punct det amod amod amod nsubj p...  \n",
       "2  nsubj case det obl case det obl flat_name punc...  \n",
       "3  nsubj root compound_prt case obl appos punct n...  \n",
       "4  case obl aux nsubj advmod det obj case det amo...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET PROPN PUNCT DET ADJ ADJ ADJ NOUN PUNCT X PUNCT PROPN PROPN PROPN PUNCT DET NOUN ADP PROPN CCONJ PRON NOUN AUX ADV VERB SCONJ PRON PUNCT ADV ADJ SCONJ ADJ PUNCT DET ADJ NOUN AUX VERB PUNCT \n",
      "DET NOUN AUX ADV ADJ ADP PUNCT\n"
     ]
    }
   ],
   "source": [
    "print (news.pos.values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 1.25 s, total: 3min 28s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.array([list(get_chains(i)) for i in youtube['text'].values]).reshape(youtube.shape[0],3).T\n",
    "youtube['lemmatized'], youtube['pos'], youtube['rel'] = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 55s, sys: 219 ms, total: 4min 56s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.array([list(get_chains(i)) for i in twitter['text'].values]).reshape(twitter.shape[0],3).T\n",
    "twitter['lemmatized'], twitter['pos'], twitter['rel'] = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('./data/news.csv', sep='\\t', index=False)\n",
    "youtube.to_csv('./data/youtube.csv', sep='\\t', index=False)\n",
    "twitter.to_csv('./data/twitter.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('./data/news.csv', sep='\\t').fillna('')\n",
    "youtube = pd.read_csv('./data/youtube.csv', sep='\\t').fillna('')\n",
    "twitter= pd.read_csv('./data/twitter.csv', sep='\\t').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...</td>\n",
       "      <td>Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...</td>\n",
       "      <td>NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...</td>\n",
       "      <td>nsubj root case amod obl appos punct case det ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>'t Endezomer  \" het laatste zomerse Meetjeslan...</td>\n",
       "      <td>het Endezomer \" het laat zomers Meetjeslands m...</td>\n",
       "      <td>DET PROPN PUNCT DET ADJ ADJ ADJ NOUN PUNCT X P...</td>\n",
       "      <td>det flat_name punct det amod amod amod nsubj p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>Kamperen bij het kanaal in de Zouten\\nBerendre...</td>\n",
       "      <td>kamperen bij het kanaal in de zouten Berendrec...</td>\n",
       "      <td>VERB ADP DET NOUN ADP DET PROPN PROPN PUNCT DE...</td>\n",
       "      <td>nsubj case det obl case det obl flat_name punc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>F</td>\n",
       "      <td>Sintdreiger komt weg met boete\\nUTRECHT - Hij ...</td>\n",
       "      <td>Sintdreiger komen weg met boete Utrecht - hij ...</td>\n",
       "      <td>PROPN VERB ADV ADP NOUN PROPN PUNCT PRON VERB ...</td>\n",
       "      <td>nsubj root compound_prt case obl appos punct n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>F</td>\n",
       "      <td>‘In Irak zal ik óók het belang van de Nederlan...</td>\n",
       "      <td>‘In Irak zullen ik óóken het belang van de Ned...</td>\n",
       "      <td>ADP PROPN AUX PRON ADJ DET NOUN ADP DET ADJ NO...</td>\n",
       "      <td>case obl aux nsubj advmod det obj case det amo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id genre gender                                               text  \\\n",
       "0   1  news      M  KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...   \n",
       "1   2  news      M  't Endezomer  \" het laatste zomerse Meetjeslan...   \n",
       "2   3  news      M  Kamperen bij het kanaal in de Zouten\\nBerendre...   \n",
       "3   4  news      F  Sintdreiger komt weg met boete\\nUTRECHT - Hij ...   \n",
       "4   5  news      F  ‘In Irak zal ik óók het belang van de Nederlan...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...   \n",
       "1  het Endezomer \" het laat zomers Meetjeslands m...   \n",
       "2  kamperen bij het kanaal in de zouten Berendrec...   \n",
       "3  Sintdreiger komen weg met boete Utrecht - hij ...   \n",
       "4  ‘In Irak zullen ik óóken het belang van de Ned...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...   \n",
       "1  DET PROPN PUNCT DET ADJ ADJ ADJ NOUN PUNCT X P...   \n",
       "2  VERB ADP DET NOUN ADP DET PROPN PROPN PUNCT DE...   \n",
       "3  PROPN VERB ADV ADP NOUN PROPN PUNCT PRON VERB ...   \n",
       "4  ADP PROPN AUX PRON ADJ DET NOUN ADP DET ADJ NO...   \n",
       "\n",
       "                                                 rel  \n",
       "0  nsubj root case amod obl appos punct case det ...  \n",
       "1  det flat_name punct det amod amod amod nsubj p...  \n",
       "2  nsubj case det obl case det obl flat_name punc...  \n",
       "3  nsubj root compound_prt case obl appos punct n...  \n",
       "4  case obl aux nsubj advmod det obj case det amo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test = xml_to_df('GxG_News_test')\n",
    "twitter_test = xml_to_df('GxG_Twitter_test')\n",
    "youtube_test = xml_to_df('GxG_YouTube_test')\n",
    "\n",
    "data = np.array([list(get_chains(i)) for i in news_test['text'].values]).reshape(news_test.shape[0],3).T\n",
    "news_test['lemmatized'], news_test['pos'], news_test['rel'] = data[0], data[1], data[2]\n",
    "\n",
    "data = np.array([list(get_chains(i)) for i in twitter_test['text'].values]).reshape(twitter_test.shape[0],3).T\n",
    "twitter_test['lemmatized'], twitter_test['pos'], twitter_test['rel'] = data[0], data[1], data[2]\n",
    "\n",
    "data = np.array([list(get_chains(i)) for i in youtube_test['text'].values]).reshape(youtube_test.shape[0],3).T\n",
    "youtube_test['lemmatized'], youtube_test['pos'], youtube_test['rel'] = data[0], data[1], data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Doodrijder juf Mare: ’Alsof er uit het niets i...</td>\n",
       "      <td>Doodrijder juf Mare : ’Alsof er uit het niets ...</td>\n",
       "      <td>NOUN PROPN PROPN PUNCT VERB ADV ADP DET PRON P...</td>\n",
       "      <td>nsubj appos flat_name punct root advmod case f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Wie zag Lady?\\n  Wetteren - Lady, een akita te...</td>\n",
       "      <td>wie zien Lady ? \\nWetteren - Lady , een akita ...</td>\n",
       "      <td>PRON VERB X PUNCT \\nVERB PUNCT X PUNCT DET NOU...</td>\n",
       "      <td>nsubj root obj punct \\nroot punct parataxis pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Lasershooten in het speelbos als alternatief k...</td>\n",
       "      <td>Lasershooten in het speelbos als alternatief k...</td>\n",
       "      <td>NOUN ADP DET NOUN ADP ADJ NOUN PROPN VERB DET ...</td>\n",
       "      <td>nsubj case det nmod mark amod nmod appos root ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Aannemers tonen geen interesse voor Torendraai...</td>\n",
       "      <td>Aannemer tonen geen interesse voor Torendraaie...</td>\n",
       "      <td>NOUN VERB DET NOUN ADP NOUN PRON VERB ADP NOUN...</td>\n",
       "      <td>nsubj root det obj case nmod nsubj acl case ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Het verkeer aan de Brielpoort bij het stemmen\\...</td>\n",
       "      <td>het verkeer aan de Brielpoort bij het stem Dei...</td>\n",
       "      <td>DET NOUN ADP DET PROPN ADP DET NOUN PROPN PUNC...</td>\n",
       "      <td>det nsubj case det nmod case det nmod appos pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id genre gender                                               text  \\\n",
       "0  1  news      ?  Doodrijder juf Mare: ’Alsof er uit het niets i...   \n",
       "1  2  news      ?  Wie zag Lady?\\n  Wetteren - Lady, een akita te...   \n",
       "2  3  news      ?  Lasershooten in het speelbos als alternatief k...   \n",
       "3  4  news      ?  Aannemers tonen geen interesse voor Torendraai...   \n",
       "4  5  news      ?  Het verkeer aan de Brielpoort bij het stemmen\\...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Doodrijder juf Mare : ’Alsof er uit het niets ...   \n",
       "1  wie zien Lady ? \\nWetteren - Lady , een akita ...   \n",
       "2  Lasershooten in het speelbos als alternatief k...   \n",
       "3  Aannemer tonen geen interesse voor Torendraaie...   \n",
       "4  het verkeer aan de Brielpoort bij het stem Dei...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  NOUN PROPN PROPN PUNCT VERB ADV ADP DET PRON P...   \n",
       "1  PRON VERB X PUNCT \\nVERB PUNCT X PUNCT DET NOU...   \n",
       "2  NOUN ADP DET NOUN ADP ADJ NOUN PROPN VERB DET ...   \n",
       "3  NOUN VERB DET NOUN ADP NOUN PRON VERB ADP NOUN...   \n",
       "4  DET NOUN ADP DET PROPN ADP DET NOUN PROPN PUNC...   \n",
       "\n",
       "                                                 rel  \n",
       "0  nsubj appos flat_name punct root advmod case f...  \n",
       "1  nsubj root obj punct \\nroot punct parataxis pu...  \n",
       "2  nsubj case det nmod mark amod nmod appos root ...  \n",
       "3  nsubj root det obj case nmod nsubj acl case ob...  \n",
       "4  det nsubj case det nmod case det nmod appos pu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def vectors(docs, model):\n",
    "    docs = [doc.split() for doc in docs]\n",
    "    docs_dict = Dictionary(docs)\n",
    "    docs_dict.filter_extremes(no_below=20, no_above=0.8)\n",
    "    docs_dict.compactify()\n",
    "\n",
    "    docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "    model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "    docs_tfidf  = model_tfidf[docs_corpus]\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "    tfidf_emb_vecs = np.vstack([model[docs_dict[i]] if docs_dict[i] in model else np.zeros(320) \n",
    "                                for i in range(len(docs_dict)) ])\n",
    "    docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "    return docs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_w2v = vectors(news.lemmatized.values, cow)\n",
    "np.save('news_w2v', news_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_w2v = vectors(youtube.lemmatized.values, cow)\n",
    "np.save('youtube_w2v', youtube_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_w2v = vectors(twitter.lemmatized.values, cow)\n",
    "np.save('twitter_w2v', twitter_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6162028189202102"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, news_w2v, news.gender.values, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5793570028644656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, youtube_w2v, youtube.gender.values, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6089"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, twitter_w2v, twitter.gender.values, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalized(text):\n",
    "    if text.startswith('@'):\n",
    "        return '@@'\n",
    "    elif text.startswith('#'):\n",
    "        return '##'\n",
    "    elif text.startswith('http') or text.startswith('www'):\n",
    "        return '~~'\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def normalized(text):\n",
    "    text = [_normalized(i) for i in tknzr.tokenize(text)]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['tokenized'] = news['text'].apply(normalized)\n",
    "twitter['tokenized'] = twitter['text'].apply(normalized)\n",
    "youtube['tokenized'] = youtube['text'].apply(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>F</td>\n",
       "      <td>Ik weet ook gewoon niet meer wie en wat ik ben...</td>\n",
       "      <td>ik weten ook gewoon niet veel wie en wat ik zi...</td>\n",
       "      <td>PRON VERB ADV ADJ ADV PRON PRON CCONJ PRON PRO...</td>\n",
       "      <td>nsubj root advmod advmod advmod obl nsubj cc c...</td>\n",
       "      <td>Ik weet ook gewoon niet meer wie en wat ik ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>M</td>\n",
       "      <td>@HaroldinhoXL Foto’s van laten maken, voor die...</td>\n",
       "      <td>@HaroldinhoXL Foto’s van laten maken , voor di...</td>\n",
       "      <td>PROPN PROPN ADP AUX VERB PUNCT ADP DET NOUN SC...</td>\n",
       "      <td>nsubj flat_name compound_prt aux root punct ca...</td>\n",
       "      <td>@@ Foto ’ s van laten maken , voor die tijd da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>M</td>\n",
       "      <td>@Rijkswaterstaat dacht dat de werken bij vught...</td>\n",
       "      <td>@Rijkswaterstaan denken dat de werk bij vught ...</td>\n",
       "      <td>AUX VERB SCONJ DET NOUN ADP NOUN ADP NUM NOUN ...</td>\n",
       "      <td>cop root mark det nsubj case obl case nummod n...</td>\n",
       "      <td>@@ dacht dat de werken bij vught tot 12 aug zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>M</td>\n",
       "      <td>Wat een zaadpot. Als dit onze concurrentie is....</td>\n",
       "      <td>wat een zaadpot . \\nals dit ons concurrentie z...</td>\n",
       "      <td>PRON DET NOUN PUNCT \\nSCONJ PRON PRON NOUN AUX...</td>\n",
       "      <td>root det nsubj punct \\nmark nsubj nmod_poss ad...</td>\n",
       "      <td>Wat een zaadpot . Als dit onze concurrentie is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>M</td>\n",
       "      <td>Ik voel een leuke trip aankomen ;-) https://t....</td>\n",
       "      <td>ik voelen een leuk trip aankomen ;- ) https://...</td>\n",
       "      <td>PRON VERB DET ADJ NOUN VERB PUNCT PUNCT SYM</td>\n",
       "      <td>nsubj root det amod obj ccomp punct punct obj</td>\n",
       "      <td>Ik voel een leuke trip aankomen ;-) ~~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    genre gender                                               text  \\\n",
       "0   1  twitter      F  Ik weet ook gewoon niet meer wie en wat ik ben...   \n",
       "1   2  twitter      M  @HaroldinhoXL Foto’s van laten maken, voor die...   \n",
       "2   3  twitter      M  @Rijkswaterstaat dacht dat de werken bij vught...   \n",
       "3   4  twitter      M  Wat een zaadpot. Als dit onze concurrentie is....   \n",
       "4   5  twitter      M  Ik voel een leuke trip aankomen ;-) https://t....   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ik weten ook gewoon niet veel wie en wat ik zi...   \n",
       "1  @HaroldinhoXL Foto’s van laten maken , voor di...   \n",
       "2  @Rijkswaterstaan denken dat de werk bij vught ...   \n",
       "3  wat een zaadpot . \\nals dit ons concurrentie z...   \n",
       "4  ik voelen een leuk trip aankomen ;- ) https://...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  PRON VERB ADV ADJ ADV PRON PRON CCONJ PRON PRO...   \n",
       "1  PROPN PROPN ADP AUX VERB PUNCT ADP DET NOUN SC...   \n",
       "2  AUX VERB SCONJ DET NOUN ADP NOUN ADP NUM NOUN ...   \n",
       "3  PRON DET NOUN PUNCT \\nSCONJ PRON PRON NOUN AUX...   \n",
       "4        PRON VERB DET ADJ NOUN VERB PUNCT PUNCT SYM   \n",
       "\n",
       "                                                 rel  \\\n",
       "0  nsubj root advmod advmod advmod obl nsubj cc c...   \n",
       "1  nsubj flat_name compound_prt aux root punct ca...   \n",
       "2  cop root mark det nsubj case obl case nummod n...   \n",
       "3  root det nsubj punct \\nmark nsubj nmod_poss ad...   \n",
       "4      nsubj root det amod obj ccomp punct punct obj   \n",
       "\n",
       "                                           tokenized  \n",
       "0  Ik weet ook gewoon niet meer wie en wat ik ben...  \n",
       "1  @@ Foto ’ s van laten maken , voor die tijd da...  \n",
       "2  @@ dacht dat de werken bij vught tot 12 aug zo...  \n",
       "3  Wat een zaadpot . Als dit onze concurrentie is...  \n",
       "4             Ik voel een leuke trip aankomen ;-) ~~  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test['tokenized'] = news_test['text'].apply(normalized)\n",
    "twitter_test['tokenized'] = twitter_test['text'].apply(normalized)\n",
    "youtube_test['tokenized'] = youtube_test['text'].apply(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Doodrijder juf Mare: ’Alsof er uit het niets i...</td>\n",
       "      <td>Doodrijder juf Mare : ’Alsof er uit het niets ...</td>\n",
       "      <td>NOUN PROPN PROPN PUNCT VERB ADV ADP DET PRON P...</td>\n",
       "      <td>nsubj appos flat_name punct root advmod case f...</td>\n",
       "      <td>Doodrijder juf Mare : ’ Alsof er uit het niets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Wie zag Lady?\\n  Wetteren - Lady, een akita te...</td>\n",
       "      <td>wie zien Lady ? \\nWetteren - Lady , een akita ...</td>\n",
       "      <td>PRON VERB X PUNCT \\nVERB PUNCT X PUNCT DET NOU...</td>\n",
       "      <td>nsubj root obj punct \\nroot punct parataxis pu...</td>\n",
       "      <td>Wie zag Lady ? Wetteren - Lady , een akita tee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Lasershooten in het speelbos als alternatief k...</td>\n",
       "      <td>Lasershooten in het speelbos als alternatief k...</td>\n",
       "      <td>NOUN ADP DET NOUN ADP ADJ NOUN PROPN VERB DET ...</td>\n",
       "      <td>nsubj case det nmod mark amod nmod appos root ...</td>\n",
       "      <td>Lasershooten in het speelbos als alternatief k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Aannemers tonen geen interesse voor Torendraai...</td>\n",
       "      <td>Aannemer tonen geen interesse voor Torendraaie...</td>\n",
       "      <td>NOUN VERB DET NOUN ADP NOUN PRON VERB ADP NOUN...</td>\n",
       "      <td>nsubj root det obj case nmod nsubj acl case ob...</td>\n",
       "      <td>Aannemers tonen geen interesse voor Torendraai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>?</td>\n",
       "      <td>Het verkeer aan de Brielpoort bij het stemmen\\...</td>\n",
       "      <td>het verkeer aan de Brielpoort bij het stem Dei...</td>\n",
       "      <td>DET NOUN ADP DET PROPN ADP DET NOUN PROPN PUNC...</td>\n",
       "      <td>det nsubj case det nmod case det nmod appos pu...</td>\n",
       "      <td>Het verkeer aan de Brielpoort bij het stemmen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id genre gender                                               text  \\\n",
       "0  1  news      ?  Doodrijder juf Mare: ’Alsof er uit het niets i...   \n",
       "1  2  news      ?  Wie zag Lady?\\n  Wetteren - Lady, een akita te...   \n",
       "2  3  news      ?  Lasershooten in het speelbos als alternatief k...   \n",
       "3  4  news      ?  Aannemers tonen geen interesse voor Torendraai...   \n",
       "4  5  news      ?  Het verkeer aan de Brielpoort bij het stemmen\\...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Doodrijder juf Mare : ’Alsof er uit het niets ...   \n",
       "1  wie zien Lady ? \\nWetteren - Lady , een akita ...   \n",
       "2  Lasershooten in het speelbos als alternatief k...   \n",
       "3  Aannemer tonen geen interesse voor Torendraaie...   \n",
       "4  het verkeer aan de Brielpoort bij het stem Dei...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  NOUN PROPN PROPN PUNCT VERB ADV ADP DET PRON P...   \n",
       "1  PRON VERB X PUNCT \\nVERB PUNCT X PUNCT DET NOU...   \n",
       "2  NOUN ADP DET NOUN ADP ADJ NOUN PROPN VERB DET ...   \n",
       "3  NOUN VERB DET NOUN ADP NOUN PRON VERB ADP NOUN...   \n",
       "4  DET NOUN ADP DET PROPN ADP DET NOUN PROPN PUNC...   \n",
       "\n",
       "                                                 rel  \\\n",
       "0  nsubj appos flat_name punct root advmod case f...   \n",
       "1  nsubj root obj punct \\nroot punct parataxis pu...   \n",
       "2  nsubj case det nmod mark amod nmod appos root ...   \n",
       "3  nsubj root det obj case nmod nsubj acl case ob...   \n",
       "4  det nsubj case det nmod case det nmod appos pu...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  Doodrijder juf Mare : ’ Alsof er uit het niets...  \n",
       "1  Wie zag Lady ? Wetteren - Lady , een akita tee...  \n",
       "2  Lasershooten in het speelbos als alternatief k...  \n",
       "3  Aannemers tonen geen interesse voor Torendraai...  \n",
       "4  Het verkeer aan de Brielpoort bij het stemmen ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_encoder(text):\n",
    "    def _cv(letter):\n",
    "        if letter in 'eyuioaàèìòùÀÈÌÒÙáéíóúýÁÉÍÓÚÝâêîôûÂÊÎÔÛãñõÃÑÕäëïöüÿÄËÏÖÜŸåÅæÆœŒøØ':\n",
    "            return 'v'\n",
    "        elif letter in 'qwrtpasdfghjklzxcvbnmQWRTPSDFGHJKLZXCVBNMçÇðÐ':\n",
    "            return 'c'\n",
    "        else:\n",
    "            return letter\n",
    "            \n",
    "    text = ''.join([_cv(letter) for letter in text])\n",
    "    return text\n",
    "\n",
    "def ul_encoder(text):\n",
    "    def _ul(letter):\n",
    "        if letter.isupper():\n",
    "            return 'U'\n",
    "        elif letter.isalpha():\n",
    "            return 'L'\n",
    "        else:\n",
    "            return letter\n",
    "    text = ''.join([_ul(letter) for letter in text])\n",
    "    return text\n",
    "\n",
    "def len_encoder(text):\n",
    "    def _len(word):\n",
    "        if word.isalpha():\n",
    "            return str(len(word))\n",
    "        else:\n",
    "            return word\n",
    "    text = text.split()\n",
    "    text = ' '.join([_len(letter) for letter in text])\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ Foto ’ s van laten maken , voor die tijd dat de badkamer vetnieuwd moet worden …\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('@@ cvcv ’ c cvc cvcvc cvcvc , cvvc cvv cvcc cvc cv cvccvcvc cvccvvvcc cvvc cvccvc …',\n",
       " '@@ ULLL ’ L LLL LLLLL LLLLL , LLLL LLL LLLL LLL LL LLLLLLLL LLLLLLLLL LLLL LLLLLL …',\n",
       " '@@ 4 ’ 1 3 5 5 , 4 3 4 3 2 8 9 4 6 …')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = twitter.tokenized.values[1]\n",
    "print (text)\n",
    "cv_encoder(text), ul_encoder(text), len_encoder(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.17 s, sys: 3.14 ms, total: 2.18 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "news['cv'] = news['tokenized'].apply(cv_encoder)\n",
    "news['ul'] = news['tokenized'].apply(ul_encoder)\n",
    "news['len'] = news['tokenized'].apply(len_encoder)\n",
    "\n",
    "twitter['cv'] = twitter['tokenized'].apply(cv_encoder)\n",
    "twitter['ul'] = twitter['tokenized'].apply(ul_encoder)\n",
    "twitter['len'] = twitter['tokenized'].apply(len_encoder)\n",
    "\n",
    "youtube['cv'] = youtube['tokenized'].apply(cv_encoder)\n",
    "youtube['ul'] = youtube['tokenized'].apply(ul_encoder)\n",
    "youtube['len'] = youtube['tokenized'].apply(len_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 12 ms, total: 1.4 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "news_test['cv'] = news_test['tokenized'].apply(cv_encoder)\n",
    "news_test['ul'] = news_test['tokenized'].apply(ul_encoder)\n",
    "news_test['len'] = news_test['tokenized'].apply(len_encoder)\n",
    "\n",
    "twitter_test['cv'] = twitter_test['tokenized'].apply(cv_encoder)\n",
    "twitter_test['ul'] = twitter_test['tokenized'].apply(ul_encoder)\n",
    "twitter_test['len'] = twitter_test['tokenized'].apply(len_encoder)\n",
    "\n",
    "youtube_test['cv'] = youtube_test['tokenized'].apply(cv_encoder)\n",
    "youtube_test['ul'] = youtube_test['tokenized'].apply(ul_encoder)\n",
    "youtube_test['len'] = youtube_test['tokenized'].apply(len_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cv</th>\n",
       "      <th>ul</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21885</td>\n",
       "      <td>youtube</td>\n",
       "      <td>?</td>\n",
       "      <td>Deze video bekijken terwijl je een vrouw bent😭...</td>\n",
       "      <td>deze video bekijken terwijl je een vrouw bent😭😂!﻿</td>\n",
       "      <td>DET NOUN VERB SCONJ PRON DET NOUN NOUN</td>\n",
       "      <td>det obj root mark nsubj det advcl nmod</td>\n",
       "      <td>Deze video bekijken terwijl je een vrouw bent ...</td>\n",
       "      <td>cvcv cvcvv cvcvccvc cvccvcc cv vvc ccvvc cvcc ...</td>\n",
       "      <td>ULLL LLLLL LLLLLLLL LLLLLLL LL LLL LLLLL LLLL ...</td>\n",
       "      <td>4 5 8 7 2 3 5 4 😭 😂 ! ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21709</td>\n",
       "      <td>youtube</td>\n",
       "      <td>?</td>\n",
       "      <td>Nee hoor een kat heeft geen maatje nodig\\nIk h...</td>\n",
       "      <td>nee hoor een kat hebben geen maat nodig \\nik h...</td>\n",
       "      <td>INTJ INTJ DET NOUN VERB DET NOUN ADJ \\nPRON AU...</td>\n",
       "      <td>obl fixed det nsubj root det obj xcomp \\nnsubj...</td>\n",
       "      <td>Nee hoor een kat heeft geen maatje nodig Ik he...</td>\n",
       "      <td>cvv cvvc vvc cvc cvvcc cvvc cvvccv cvcvc Ic cv...</td>\n",
       "      <td>ULL LLLL LLL LLL LLLLL LLLL LLLLLL LLLLL UL LL...</td>\n",
       "      <td>3 4 3 3 5 4 6 5 2 3 3 3 3 2 2 2 2 5 4 6 ﻿ 5 7 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18586</td>\n",
       "      <td>youtube</td>\n",
       "      <td>?</td>\n",
       "      <td>Subb op mij﻿\\n</td>\n",
       "      <td>Subb op mij﻿</td>\n",
       "      <td>AUX ADP NOUN</td>\n",
       "      <td>root case obl</td>\n",
       "      <td>Subb op mij ﻿</td>\n",
       "      <td>cvcc vc cvc ﻿</td>\n",
       "      <td>ULLL LL LLL ﻿</td>\n",
       "      <td>4 2 3 ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28421</td>\n",
       "      <td>youtube</td>\n",
       "      <td>?</td>\n",
       "      <td>Mijn irritatie us dat er te veel kleine kinder...</td>\n",
       "      <td>mijn irritatie us dat er te veel klein kind zi...</td>\n",
       "      <td>PRON NOUN ADJ SCONJ ADV ADV PRON ADJ NOUN AUX ...</td>\n",
       "      <td>nmod_poss nsubj amod mark advmod advmod advmod...</td>\n",
       "      <td>Mijn irritatie us dat er te veel kleine kinder...</td>\n",
       "      <td>cvcc vccvcvcvv vc cvc vc cv cvvc ccvvcv cvccvc...</td>\n",
       "      <td>ULLL LLLLLLLLL LL LLL LL LL LLLL LLLLLL LLLLLL...</td>\n",
       "      <td>4 9 2 3 2 2 4 6 8 4 3 5 7 4 ﻿ 2 3 2 4 5 6 ﻿ 6 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26799</td>\n",
       "      <td>youtube</td>\n",
       "      <td>?</td>\n",
       "      <td>Wie heeft er ook geen schoenen aan doe dan ff ...</td>\n",
       "      <td>wie hebben er ook geen schoen aan doen dan ff ...</td>\n",
       "      <td>PRON AUX ADV ADV DET NOUN ADP NOUN ADV ADJ DET...</td>\n",
       "      <td>nsubj root advmod advmod det obj case obl advm...</td>\n",
       "      <td>Wie heeft er ook geen schoenen aan doe dan ff ...</td>\n",
       "      <td>cvv cvvcc vc vvc cvvc cccvvcvc vvc cvv cvc cc ...</td>\n",
       "      <td>ULL LLLLL LL LLL LLLL LLLLLLLL LLL LLL LLL LL ...</td>\n",
       "      <td>3 5 2 3 4 8 3 3 3 2 3 4 🙏 🏿 ﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    genre gender                                               text  \\\n",
       "0  21885  youtube      ?  Deze video bekijken terwijl je een vrouw bent😭...   \n",
       "1  21709  youtube      ?  Nee hoor een kat heeft geen maatje nodig\\nIk h...   \n",
       "2  18586  youtube      ?                                     Subb op mij﻿\\n   \n",
       "3  28421  youtube      ?  Mijn irritatie us dat er te veel kleine kinder...   \n",
       "4  26799  youtube      ?  Wie heeft er ook geen schoenen aan doe dan ff ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  deze video bekijken terwijl je een vrouw bent😭😂!﻿   \n",
       "1  nee hoor een kat hebben geen maat nodig \\nik h...   \n",
       "2                                       Subb op mij﻿   \n",
       "3  mijn irritatie us dat er te veel klein kind zi...   \n",
       "4  wie hebben er ook geen schoen aan doen dan ff ...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0             DET NOUN VERB SCONJ PRON DET NOUN NOUN   \n",
       "1  INTJ INTJ DET NOUN VERB DET NOUN ADJ \\nPRON AU...   \n",
       "2                                       AUX ADP NOUN   \n",
       "3  PRON NOUN ADJ SCONJ ADV ADV PRON ADJ NOUN AUX ...   \n",
       "4  PRON AUX ADV ADV DET NOUN ADP NOUN ADV ADJ DET...   \n",
       "\n",
       "                                                 rel  \\\n",
       "0             det obj root mark nsubj det advcl nmod   \n",
       "1  obl fixed det nsubj root det obj xcomp \\nnsubj...   \n",
       "2                                      root case obl   \n",
       "3  nmod_poss nsubj amod mark advmod advmod advmod...   \n",
       "4  nsubj root advmod advmod det obj case obl advm...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  Deze video bekijken terwijl je een vrouw bent ...   \n",
       "1  Nee hoor een kat heeft geen maatje nodig Ik he...   \n",
       "2                                      Subb op mij ﻿   \n",
       "3  Mijn irritatie us dat er te veel kleine kinder...   \n",
       "4  Wie heeft er ook geen schoenen aan doe dan ff ...   \n",
       "\n",
       "                                                  cv  \\\n",
       "0  cvcv cvcvv cvcvccvc cvccvcc cv vvc ccvvc cvcc ...   \n",
       "1  cvv cvvc vvc cvc cvvcc cvvc cvvccv cvcvc Ic cv...   \n",
       "2                                      cvcc vc cvc ﻿   \n",
       "3  cvcc vccvcvcvv vc cvc vc cv cvvc ccvvcv cvccvc...   \n",
       "4  cvv cvvcc vc vvc cvvc cccvvcvc vvc cvv cvc cc ...   \n",
       "\n",
       "                                                  ul  \\\n",
       "0  ULLL LLLLL LLLLLLLL LLLLLLL LL LLL LLLLL LLLL ...   \n",
       "1  ULL LLLL LLL LLL LLLLL LLLL LLLLLL LLLLL UL LL...   \n",
       "2                                      ULLL LL LLL ﻿   \n",
       "3  ULLL LLLLLLLLL LL LLL LL LL LLLL LLLLLL LLLLLL...   \n",
       "4  ULL LLLLL LL LLL LLLL LLLLLLLL LLL LLL LLL LL ...   \n",
       "\n",
       "                                                 len  \n",
       "0                            4 5 8 7 2 3 5 4 😭 😂 ! ﻿  \n",
       "1  3 4 3 3 5 4 6 5 2 3 3 3 3 2 2 2 2 5 4 6 ﻿ 5 7 ...  \n",
       "2                                            4 2 3 ﻿  \n",
       "3  4 9 2 3 2 2 4 6 8 4 3 5 7 4 ﻿ 2 3 2 4 5 6 ﻿ 6 ...  \n",
       "4                      3 5 2 3 4 8 3 3 3 2 3 4 🙏 🏿 ﻿  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cv</th>\n",
       "      <th>ul</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22988</td>\n",
       "      <td>youtube</td>\n",
       "      <td>M</td>\n",
       "      <td>12:55 stickers van zijn lelijke kut hoofd😂😂﻿\\n</td>\n",
       "      <td>12:55 sticker van zijn lelijk kut hoofd😂😂﻿</td>\n",
       "      <td>NUM NOUN ADP PRON ADJ NOUN NOUN</td>\n",
       "      <td>nummod nsubj case nmod_poss amod nmod root</td>\n",
       "      <td>12:55 stickers van zijn lelijke kut hoofd 😂 😂 ﻿</td>\n",
       "      <td>12:55 ccvccvcc cvc cvcc cvcvccv cvc cvvcc 😂 😂 ﻿</td>\n",
       "      <td>12:55 LLLLLLLL LLL LLLL LLLLLLL LLL LLLLL 😂 😂 ﻿</td>\n",
       "      <td>12:55 8 3 4 7 3 5 😂 😂 ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22192</td>\n",
       "      <td>youtube</td>\n",
       "      <td>M</td>\n",
       "      <td>Top video! ;) Jullie maken letterlijk de aller...</td>\n",
       "      <td>top video ! ; ) Jullië maken letterlijk de all...</td>\n",
       "      <td>ADP NOUN PUNCT PUNCT PUNCT PRON VERB ADJ DET A...</td>\n",
       "      <td>case parataxis punct punct punct nsubj paratax...</td>\n",
       "      <td>Top video ! ;) Jullie maken letterlijk de alle...</td>\n",
       "      <td>cvc cvcvv ! ;) cvccvv cvcvc cvccvccvcc cv vccv...</td>\n",
       "      <td>ULL LLLLL ! ;) ULLLLL LLLLL LLLLLLLLLL LL LLLL...</td>\n",
       "      <td>3 5 ! ;) 6 5 10 2 12 6 3 2 4 3 4 &gt; ... ﻿ 3 4 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    genre gender                                               text  \\\n",
       "0  22988  youtube      M     12:55 stickers van zijn lelijke kut hoofd😂😂﻿\\n   \n",
       "1  22192  youtube      M  Top video! ;) Jullie maken letterlijk de aller...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0         12:55 sticker van zijn lelijk kut hoofd😂😂﻿   \n",
       "1  top video ! ; ) Jullië maken letterlijk de all...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0                    NUM NOUN ADP PRON ADJ NOUN NOUN   \n",
       "1  ADP NOUN PUNCT PUNCT PUNCT PRON VERB ADJ DET A...   \n",
       "\n",
       "                                                 rel  \\\n",
       "0         nummod nsubj case nmod_poss amod nmod root   \n",
       "1  case parataxis punct punct punct nsubj paratax...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0    12:55 stickers van zijn lelijke kut hoofd 😂 😂 ﻿   \n",
       "1  Top video ! ;) Jullie maken letterlijk de alle...   \n",
       "\n",
       "                                                  cv  \\\n",
       "0    12:55 ccvccvcc cvc cvcc cvcvccv cvc cvvcc 😂 😂 ﻿   \n",
       "1  cvc cvcvv ! ;) cvccvv cvcvc cvccvccvcc cv vccv...   \n",
       "\n",
       "                                                  ul  \\\n",
       "0    12:55 LLLLLLLL LLL LLLL LLLLLLL LLL LLLLL 😂 😂 ﻿   \n",
       "1  ULL LLLLL ! ;) ULLLLL LLLLL LLLLLLLLLL LL LLLL...   \n",
       "\n",
       "                                                 len  \n",
       "0                            12:55 8 3 4 7 3 5 😂 😂 ﻿  \n",
       "1  3 5 ! ;) 6 5 10 2 12 6 3 2 4 3 4 > ... ﻿ 3 4 2...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('./data/news.csv', sep='\\t', index=False)\n",
    "youtube.to_csv('./data/youtube.csv', sep='\\t', index=False)\n",
    "twitter.to_csv('./data/twitter.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test.to_csv('./data/news_test.csv', sep='\\t', index=False)\n",
    "youtube_test.to_csv('./data/youtube_test.csv', sep='\\t', index=False)\n",
    "twitter_test.to_csv('./data/twitter_test.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(data, classifier, dense=False):\n",
    "    for i in ['lemmatized','pos','rel','cv','ul','len']:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', token_pattern='\\S+', ngram_range=(1, 4), \n",
    "                             max_df=0.9, min_df=5, max_features=20000)\n",
    "\n",
    "        vectorizer.fit(data[i].values)\n",
    "\n",
    "        X = vectorizer.transform(data[i].values)\n",
    "        if dense: X = X.toarray()\n",
    "        #print (X.shape)\n",
    "        y = [1 if i=='F' else 0 for i in data.gender.values]\n",
    "\n",
    "        clf = clone(classifier)\n",
    "        score = cross_val_score(clf, X, y, cv=10).mean()\n",
    "        print (i, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news\n",
      "lemmatized 0.6653786430960345\n",
      "pos 0.5742713807931199\n",
      "rel 0.5786192068800764\n",
      "cv 0.6254837553750596\n",
      "ul 0.5976827520305781\n",
      "len 0.5781115623506927\n",
      "\n",
      "twitter\n",
      "lemmatized 0.6215\n",
      "pos 0.5677\n",
      "rel 0.55305\n",
      "cv 0.5866999999999999\n",
      "ul 0.59005\n",
      "len 0.5809\n",
      "\n",
      "youtube\n",
      "lemmatized 0.5980753108073822\n",
      "pos 0.5479533228168104\n",
      "rel 0.5384574724308979\n",
      "cv 0.6058766588344309\n",
      "ul 0.5830871510886071\n",
      "len 0.5836292484363107\n",
      "CPU times: user 4min 6s, sys: 5min 42s, total: 9min 48s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "print ('news')\n",
    "run_all(news, clf)\n",
    "print ('\\ntwitter')\n",
    "run_all(twitter, clf)\n",
    "print ('\\nyoutube')\n",
    "run_all(youtube, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(random_state=23)\n",
    "print ('news')\n",
    "run_all(news, clf)\n",
    "print ('\\ntwitter')\n",
    "run_all(twitter, clf)\n",
    "print ('\\nyoutube')\n",
    "run_all(youtube, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "clf = CatBoostClassifier(learning_rate=1, #тоже верхняя граница\n",
    "                           depth=6, #это верхняя граница, если сделать выше, то с малым lr обучается оч долго\n",
    "                           iterations=100, #с запасом, вряд ли столько нужно\n",
    "                           random_state=23, #сид\n",
    "                           early_stopping_rounds=10, #столько итераций с запасом, лучше остановиться\n",
    "                           loss_function='Logloss', \n",
    "                           custom_loss='F1',\n",
    "                           \n",
    "                           verbose=0) #без вывода\n",
    "print ('news')\n",
    "run_all(news, clf, dense=True)\n",
    "print ('twitter')\n",
    "run_all(twitter, clf, dense=True)\n",
    "print ('youtube')\n",
    "run_all(youtube, clf, dense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "def vectors(docs, model, target):\n",
    "    docs = [doc.split() for doc in docs]\n",
    "    docs_dict = Dictionary(docs)\n",
    "    docs_dict.filter_extremes(no_below=5, no_above=0.8)\n",
    "    docs_dict.compactify()\n",
    "    \n",
    "    target = [doc.split() for doc in target]\n",
    "\n",
    "    docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "    docs_target = [docs_dict.doc2bow(doc) for doc in target] #target\n",
    "    model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "    docs_tfidf  = model_tfidf[docs_target] #target\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "    tfidf_emb_vecs = np.vstack([model[docs_dict[i]] if docs_dict[i] in model else np.zeros(320) \n",
    "                                for i in range(len(docs_dict)) ])\n",
    "    X_train = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "    \n",
    "    docs_tfidf  = model_tfidf[docs_corpus]\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "    tfidf_emb_vecs = np.vstack([model[docs_dict[i]] if docs_dict[i] in model else np.zeros(320) \n",
    "                                for i in range(len(docs_dict)) ])\n",
    "    X_test = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('./data/news.csv', sep='\\t').fillna('')\n",
    "youtube = pd.read_csv('./data/youtube.csv', sep='\\t').fillna('')\n",
    "twitter= pd.read_csv('./data/twitter.csv', sep='\\t').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News In-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos</th>\n",
       "      <th>rel</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cv</th>\n",
       "      <th>ul</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>news</td>\n",
       "      <td>M</td>\n",
       "      <td>KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...</td>\n",
       "      <td>Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...</td>\n",
       "      <td>NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...</td>\n",
       "      <td>nsubj root case amod obl appos punct case det ...</td>\n",
       "      <td>KVV begint aan nieuw voetbalhoofdstuk Zelzate ...</td>\n",
       "      <td>ccc cvcvcc vvc cvvvc cvvccvccvvccccvc cvccvcv ...</td>\n",
       "      <td>UUU LLLLLL LLL LLLLL LLLLLLLLLLLLLLLL ULLLLLL ...</td>\n",
       "      <td>3 6 3 5 16 7 - 2 2 7 3 6 2 2 8 6 5 3 7 3 2 17 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id genre gender                                               text  \\\n",
       "0   1  news      M  KVV begint aan nieuw voetbalhoofdstuk\\nZelzate...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Kvv beginnen aan nieuw voetbalhoofdstuk Zelzat...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  NOUN VERB ADP ADJ NOUN PROPN PUNCT ADP DET PRO...   \n",
       "\n",
       "                                                 rel  \\\n",
       "0  nsubj root case amod obl appos punct case det ...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  KVV begint aan nieuw voetbalhoofdstuk Zelzate ...   \n",
       "\n",
       "                                                  cv  \\\n",
       "0  ccc cvcvcc vvc cvvvc cvvccvccvvccccvc cvccvcv ...   \n",
       "\n",
       "                                                  ul  \\\n",
       "0  UUU LLLLLL LLL LLLLL LLLLLLLLLLLLLLLL ULLLLLL ...   \n",
       "\n",
       "                                                 len  \n",
       "0  3 6 3 5 16 7 - 2 2 7 3 6 2 2 8 6 5 3 7 3 2 17 ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_w2v = vectors(news.lemmatized.values, cow)\n",
    "#np.save('news_w2v', news_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline1(data, target_df, model):\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    for i in tqdm(['lemmatized','pos','rel','cv','ul','len']):\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', token_pattern='\\S+', ngram_range=(1, 2), \n",
    "                             max_df=0.9, min_df=5, max_features=20000)\n",
    "        vectorizer.fit(data[i].values)\n",
    "        X_t = vectorizer.transform(target_df[i].values)#.toarray()\n",
    "        if X_train is not None:\n",
    "            X_train = hstack([X_train, X_t])\n",
    "        else:\n",
    "            X_train = X_t\n",
    "            \n",
    "        X_t = vectorizer.transform(data[i].values)#.toarray()\n",
    "        if X_test is not None:\n",
    "            X_test = hstack([X_test, X_t])\n",
    "        else:\n",
    "            X_test = X_t\n",
    "    \n",
    "    for i in tqdm(['tokenized', 'cv','ul']):\n",
    "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 4), \n",
    "                             max_df=0.9, min_df=5, max_features=20000)\n",
    "        vectorizer.fit(data[i].values)\n",
    "        X_t = vectorizer.transform(target_df[i].values)#.toarray()\n",
    "        X_train = hstack([X_train, X_t])\n",
    "        \n",
    "        X_t = vectorizer.transform(data[i].values)#.toarray()\n",
    "        X_test = hstack([X_test, X_t])\n",
    "            \n",
    "    X_t, X_t2 = vectors(data.lemmatized.values, model, target_df.lemmatized.values)\n",
    "    X_train = hstack([X_train, X_t])\n",
    "    X_test = hstack([X_test, X_t2])\n",
    "    \n",
    "    y_train = [1 if i=='F' else 0 for i in data.gender.values]\n",
    "    y_test = [1 if i=='F' else 0 for i in target_df.gender.values]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11df872560064946b70a652b92a30fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75fa192b92e46e4949d304d9cb7ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 56.8 s, sys: 9.59 s, total: 1min 6s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = pipeline1(news, news, cow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506474221905441"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline2(data, target_df, model):\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    for i in tqdm(['pos','rel','cv','ul','len']):\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', token_pattern='\\S+', ngram_range=(1, 4), \n",
    "                             max_df=0.9, min_df=5, max_features=20000)\n",
    "        vectorizer.fit(data[i].values)\n",
    "        X_t = vectorizer.transform(target_df[i].values)\n",
    "        if X_train is not None:\n",
    "            X_train = hstack([X_train, X_t])\n",
    "        else:\n",
    "            X_train = X_t\n",
    "            \n",
    "        X_t = vectorizer.transform(data[i].values)\n",
    "        if X_test is not None:\n",
    "            X_test = hstack([X_test, X_t])\n",
    "        else:\n",
    "            X_test = X_t\n",
    "    \n",
    "    for i in tqdm(['tokenized', 'cv','ul']):\n",
    "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 4), \n",
    "                             max_df=0.9, min_df=5, max_features=20000)\n",
    "        vectorizer.fit(data[i].values)\n",
    "        X_t = vectorizer.transform(target_df[i].values)\n",
    "        X_train = hstack([X_train, X_t])\n",
    "        \n",
    "        X_t = vectorizer.transform(data[i].values)\n",
    "        X_test = hstack([X_test, X_t])\n",
    "            \n",
    "    #X_t, X_t2 = vectors(data.lemmatized.values, model, target_df.lemmatized.values)\n",
    "    #X_train = np.hstack([X_train, X_t])\n",
    "    #X_test = np.hstack([X_test, X_t2])\n",
    "    \n",
    "    y_train = [1 if i=='F' else 0 for i in data.gender.values]\n",
    "    y_test = [1 if i=='F' else 0 for i in target_df.gender.values]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb83a6c8144d87b7a6c563439967bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaa0647b08042269ae566ab67c925bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6191000000000001"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = pipeline1(twitter, twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788d2592afa4ef49338e8bf2deba8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b62ddde11d47a7ace57bc282e43cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6073999999999999"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = pipeline2(twitter, twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News cross-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd65e623d64c5481118844d9c84dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e793d39646145ffacb1de4ea468ab94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.537117903930131"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([twitter, youtube]), news, cow)\n",
    "\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e5822341804cbb89ef4f0ba327565e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6c6622239840a1a3dd62b4c30c4c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5338427947598253"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([twitter, youtube]), news, cow)\n",
    "\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter in-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9713eb1685f74f32a0329c52c1e5af65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86982b918424efcb72ff31f2e20648b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6191000000000001"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline1(twitter, twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6753dd9218c84e87956fbf4642adbd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d5104e6c5a47c3965ceb5b44987bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6073999999999999"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "X_test, X_train, y_train, y_test = pipeline2(twitter, twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter cross-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e035df8974db4f85b96c535bd9d6bfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acbf74ce1144a61a079fd8cd872ba96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5612"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([news, youtube]), twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93cab2573654e248f338284d30446f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc13976ea9c6431695cd4c32092806c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.54995"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([news, youtube]), twitter, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube in-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6b692f5f8042aa820d76daa5038387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5905f4cd43de456e974d770d181590f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6169976312581928"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "X_test, X_train, y_train, y_test = pipeline1(youtube, youtube, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1e70d698cc47a0aa2c570baaf8a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7752ad847ba4bd6ba70254d73e79ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6160499505553895"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "X_test, X_train, y_train, y_test = pipeline2(youtube, youtube, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "cross_val_score(clf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube cross-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d582fa029ed4fd69a19e243a56dabef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ce7980398143dfbb633afcd6dd2b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5332338578404775"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([news, twitter]), youtube, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267b980e525c4c0eb3bfdc4c3e6ef937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91924c9f46d462b966d0cd5186c0e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5356755290287575"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test, y_train, y_test\n",
    "\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([news, twitter]), youtube, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1faf4d44104f898cbfca4643014c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12542b2ffcc74656897565ce137261ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a7b3062fe9484fa6f7d797ba803bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bc1bee3e2a4ca7b653f92b65ca05ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b1901523b417e9c37d1b4aefa6500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f808e7c3864749a22e84aca533bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4612ff2112840468e371ca9b66251d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c609f09e72c94618849033a3aa7a577d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#news in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(news, news_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':news_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_news_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#news in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(news, news_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':news_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_news_2', index=False, header=False, sep=' ')\n",
    "\n",
    "#news cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([twitter, youtube]), news_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':news_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_news_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#news cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([twitter, youtube]), news_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':news_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_news_2', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5c2f2c5a684a2f950352d26273d381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf172df2d836458c88a75c05586ec4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37db7e2d387941509a12809797b639cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9d2dd07a2a447283348e25408ed23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230f48be5b2d44c390b0c658e02abd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064a770e237d41a69d4e78e68509884c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7262ca2cf4e4e40b387a6d82ac3b822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89cf363df954dbeb34ae4189db2ba4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#twitter in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(twitter, twitter_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':twitter_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_twitter_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#twitter in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(twitter, twitter_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':twitter_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_twitter_2', index=False, header=False, sep=' ')\n",
    "\n",
    "#twitter cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([news, youtube]), twitter_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':twitter_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_twitter_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#twitter cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([news, youtube]), twitter_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':twitter_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_twitter_2', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b2df56d3f14d7caa21d15fa17986e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441a1f7d049d49ac95deafcfc039fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4543a2895bec4074bbf2bb789ff954db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6f9eaeeb994758b1465681742eb3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eb416189af4c1399b74a78cb51f83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a18aa03fb948d2a5c97305e4a13be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fa3102148c4e5788441ff111e6019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fc7f86746d4014bc5fab16219cf454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#youtube in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(youtube, youtube_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':youtube_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_youtube_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#youtube in-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(youtube, youtube_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':youtube_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_IN_youtube_2', index=False, header=False, sep=' ')\n",
    "\n",
    "#youtube cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline1(pd.concat([news, twitter]), youtube_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':youtube_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_youtube_1', index=False, header=False, sep=' ')\n",
    "\n",
    "#youtube cross-genre\n",
    "X_test, X_train, y_train, y_test = pipeline2(pd.concat([news, twitter]), youtube_test, cow)\n",
    "clf = LogisticRegression(random_state=23, solver='lbfgs', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "result = ['F' if i ==1 else 'M' for i in clf.predict(X_test)]\n",
    "result = pd.DataFrame({'id':youtube_test.id, 'z_G':result})\n",
    "result.to_csv('Glazunov_CROSS_youtube_2', index=False, header=False, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
